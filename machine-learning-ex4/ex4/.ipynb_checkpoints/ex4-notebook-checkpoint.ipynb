{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans = /Users/Evan/Documents/Fall_2018/Machine Learning - Coursera - Stanford/Week 5/Assignment 4/machine-learning-ex4/ex4\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%% Machine Learning Online Class - Exercise 4 Neural Network Learning\n",
      "\n",
      "%  Instructions\n",
      "%  ------------\n",
      "% \n",
      "%  This file contains code that helps you get started on the\n",
      "%  linear exercise. You will need to complete the following functions \n",
      "%  in this exericse:\n",
      "%\n",
      "%     sigmoidGradient.m\n",
      "%     randInitializeWeights.m\n",
      "%     nnCostFunction.m\n",
      "%\n",
      "%  For this exercise, you will not need to change any code in this file,\n",
      "%  or any other files other than those mentioned above.\n",
      "%\n",
      "\n",
      "%% Initialization\n",
      "clear ; close all; clc\n",
      "\n",
      "%% Setup the parameters you will use for this exercise\n",
      "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
      "hidden_layer_size = 25;   % 25 hidden units\n",
      "num_labels = 10;          % 10 labels, from 1 to 10   \n",
      "                          % (note that we have mapped \"0\" to label 10)\n",
      "\n",
      "%% =========== Part 1: Loading and Visualizing Data =============\n",
      "%  We start the exercise by first loading and visualizing the dataset. \n",
      "%  You will be working with a dataset that contains handwritten digits.\n",
      "%\n",
      "\n",
      "% Load Training Data\n",
      "fprintf('Loading and Visualizing Data ...\\n')\n",
      "\n",
      "load('ex4data1.mat');\n",
      "m = size(X, 1);\n",
      "\n",
      "% Randomly select 100 data points to display\n",
      "sel = randperm(size(X, 1));\n",
      "sel = sel(1:100);\n",
      "\n",
      "displayData(X(sel, :));\n",
      "\n",
      "fprintf('Program paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "\n",
      "%% ================ Part 2: Loading Parameters ================\n",
      "% In this part of the exercise, we load some pre-initialized \n",
      "% neural network parameters.\n",
      "\n",
      "fprintf('\\nLoading Saved Neural Network Parameters ...\\n')\n",
      "\n",
      "% Load the weights into variables Theta1 and Theta2\n",
      "load('ex4weights.mat');\n",
      "\n",
      "% Unroll parameters \n",
      "nn_params = [Theta1(:) ; Theta2(:)];\n",
      "\n",
      "%% ================ Part 3: Compute Cost (Feedforward) ================\n",
      "%  To the neural network, you should first start by implementing the\n",
      "%  feedforward part of the neural network that returns the cost only. You\n",
      "%  should complete the code in nnCostFunction.m to return cost. After\n",
      "%  implementing the feedforward to compute the cost, you can verify that\n",
      "%  your implementation is correct by verifying that you get the same cost\n",
      "%  as us for the fixed debugging parameters.\n",
      "%\n",
      "%  We suggest implementing the feedforward cost *without* regularization\n",
      "%  first so that it will be easier for you to debug. Later, in part 4, you\n",
      "%  will get to implement the regularized cost.\n",
      "%\n",
      "fprintf('\\nFeedforward Using Neural Network ...\\n')\n",
      "\n",
      "% Weight regularization parameter (we set this to 0 here).\n",
      "lambda = 0;\n",
      "\n",
      "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
      "                   num_labels, X, y, lambda);\n",
      "\n",
      "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
      "         '\\n(this value should be about 0.287629)\\n'], J);\n",
      "\n",
      "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "%% =============== Part 4: Implement Regularization ===============\n",
      "%  Once your cost function implementation is correct, you should now\n",
      "%  continue to implement the regularization with the cost.\n",
      "%\n",
      "\n",
      "fprintf('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
      "\n",
      "% Weight regularization parameter (we set this to 1 here).\n",
      "lambda = 1;\n",
      "\n",
      "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
      "                   num_labels, X, y, lambda);\n",
      "\n",
      "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
      "         '\\n(this value should be about 0.383770)\\n'], J);\n",
      "\n",
      "fprintf('Program paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "\n",
      "%% ================ Part 5: Sigmoid Gradient  ================\n",
      "%  Before you start implementing the neural network, you will first\n",
      "%  implement the gradient for the sigmoid function. You should complete the\n",
      "%  code in the sigmoidGradient.m file.\n",
      "%\n",
      "\n",
      "fprintf('\\nEvaluating sigmoid gradient...\\n')\n",
      "\n",
      "g = sigmoidGradient([-1 -0.5 0 0.5 1]);\n",
      "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
      "fprintf('%f ', g);\n",
      "fprintf('\\n\\n');\n",
      "\n",
      "fprintf('Program paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "\n",
      "%% ================ Part 6: Initializing Pameters ================\n",
      "%  In this part of the exercise, you will be starting to implment a two\n",
      "%  layer neural network that classifies digits. You will start by\n",
      "%  implementing a function to initialize the weights of the neural network\n",
      "%  (randInitializeWeights.m)\n",
      "\n",
      "fprintf('\\nInitializing Neural Network Parameters ...\\n')\n",
      "\n",
      "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
      "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
      "\n",
      "% Unroll parameters\n",
      "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];\n",
      "\n",
      "\n",
      "%% =============== Part 7: Implement Backpropagation ===============\n",
      "%  Once your cost matches up with ours, you should proceed to implement the\n",
      "%  backpropagation algorithm for the neural network. You should add to the\n",
      "%  code you've written in nnCostFunction.m to return the partial\n",
      "%  derivatives of the parameters.\n",
      "%\n",
      "fprintf('\\nChecking Backpropagation... \\n');\n",
      "\n",
      "%  Check gradients by running checkNNGradients\n",
      "checkNNGradients;\n",
      "\n",
      "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "\n",
      "%% =============== Part 8: Implement Regularization ===============\n",
      "%  Once your backpropagation implementation is correct, you should now\n",
      "%  continue to implement the regularization with the cost and gradient.\n",
      "%\n",
      "\n",
      "fprintf('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
      "\n",
      "%  Check gradients by running checkNNGradients\n",
      "lambda = 3;\n",
      "checkNNGradients(lambda);\n",
      "\n",
      "% Also output the costFunction debugging values\n",
      "debug_J  = nnCostFunction(nn_params, input_layer_size, ...\n",
      "                          hidden_layer_size, num_labels, X, y, lambda);\n",
      "\n",
      "fprintf(['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' ...\n",
      "         '\\n(for lambda = 3, this value should be about 0.576051)\\n\\n'], lambda, debug_J);\n",
      "\n",
      "fprintf('Program paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "\n",
      "%% =================== Part 8: Training NN ===================\n",
      "%  You have now implemented all the code necessary to train a neural \n",
      "%  network. To train your neural network, we will now use \"fmincg\", which\n",
      "%  is a function which works similarly to \"fminunc\". Recall that these\n",
      "%  advanced optimizers are able to train our cost functions efficiently as\n",
      "%  long as we provide them with the gradient computations.\n",
      "%\n",
      "fprintf('\\nTraining Neural Network... \\n')\n",
      "\n",
      "%  After you have completed the assignment, change the MaxIter to a larger\n",
      "%  value to see how more training helps.\n",
      "options = optimset('MaxIter', 50);\n",
      "\n",
      "%  You should also try different values of lambda\n",
      "lambda = 1;\n",
      "\n",
      "% Create \"short hand\" for the cost function to be minimized\n",
      "costFunction = @(p) nnCostFunction(p, ...\n",
      "                                   input_layer_size, ...\n",
      "                                   hidden_layer_size, ...\n",
      "                                   num_labels, X, y, lambda);\n",
      "\n",
      "% Now, costFunction is a function that takes in only one argument (the\n",
      "% neural network parameters)\n",
      "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
      "\n",
      "% Obtain Theta1 and Theta2 back from nn_params\n",
      "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
      "                 hidden_layer_size, (input_layer_size + 1));\n",
      "\n",
      "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
      "                 num_labels, (hidden_layer_size + 1));\n",
      "\n",
      "fprintf('Program paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "\n",
      "%% ================= Part 9: Visualize Weights =================\n",
      "%  You can now \"visualize\" what the neural network is learning by \n",
      "%  displaying the hidden units to see what features they are capturing in \n",
      "%  the data.\n",
      "\n",
      "fprintf('\\nVisualizing Neural Network... \\n')\n",
      "\n",
      "displayData(Theta1(:, 2:end));\n",
      "\n",
      "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
      "pause;\n",
      "\n",
      "%% ================= Part 10: Implement Predict =================\n",
      "%  After training the neural network, we would like to use it to predict\n",
      "%  the labels. You will now implement the \"predict\" function to use the\n",
      "%  neural network to predict the labels of the training set. This lets\n",
      "%  you compute the training set accuracy.\n",
      "\n",
      "pred = predict(Theta1, Theta2, X);\n",
      "\n",
      "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);\n",
      "\n",
      "\n",
      "ans = 0\n"
     ]
    }
   ],
   "source": [
    "system('cat ex4.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables in the current scope:\n",
      "\n",
      "   Attr Name                   Size                     Bytes  Class\n",
      "   ==== ====                   ====                     =====  ===== \n",
      "        X                   5000x400                 16000000  double\n",
      "        ans                    1x1                          8  double\n",
      "        hidden_layer_size      1x1                          8  double\n",
      "        input_layer_size       1x1                          8  double\n",
      "        m                      1x1                          8  double\n",
      "        num_labels             1x1                          8  double\n",
      "        y                   5000x1                      40000  double\n",
      "\n",
      "Total is 2005005 elements using 16040040 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "load('ex4data1.mat')\n",
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  5000\n"
     ]
    }
   ],
   "source": [
    "m = size(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('ex4weights.mat');\n",
    "\n",
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables in the current scope:\n",
      "\n",
      "   Attr Name                   Size                     Bytes  Class\n",
      "   ==== ====                   ====                     =====  ===== \n",
      "        Theta1                25x401                    80200  double\n",
      "        Theta2                10x26                      2080  double\n",
      "        X                   5000x400                 16000000  double\n",
      "        ans                    1x1                          8  double\n",
      "        hidden_layer_size      1x1                          8  double\n",
      "        input_layer_size       1x1                          8  double\n",
      "        m                      1x1                          8  double\n",
      "        nn_params          10285x1                      82280  double\n",
      "        num_labels             1x1                          8  double\n",
      "        y                   5000x1                      40000  double\n",
      "\n",
      "Total is 2025575 elements using 16204600 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse error:\n",
      "\n",
      "  syntax error\n",
      "\n",
      ">>> delta_1 = (Theta1' * delta_two) .* (1 - )\n",
      "                                           ^\n",
      "\n",
      "error: 'Theta1_grad' undefined near line 2 column 9\n",
      "parse error:\n",
      "\n",
      "  syntax error\n",
      "\n",
      ">>> end\n",
      "      ^\n",
      "\n",
      "ans =\n",
      "\n",
      "   5000     10\n",
      "\n",
      "ans =\n",
      "\n",
      "   5000     10\n",
      "\n",
      "regularization_term =  0.096141\n",
      "Cost at parameters (loaded from ex4weights): 0.383770 \n",
      "(this value should be about 0.287629)\n"
     ]
    }
   ],
   "source": [
    "% Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 0;\n",
    "\n",
    "function [J grad] = nnCostFunction(nn_params, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, ...\n",
    "                                   X, y, lambda)\n",
    "%NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "%neural network which performs classification\n",
    "%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "%   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "%   parameters for the neural network are \"unrolled\" into the vector\n",
    "%   nn_params and need to be converted back into the weight matrices. \n",
    "% \n",
    "%   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "%   partial derivatives of the neural network.\n",
    "%\n",
    "\n",
    "%{\n",
    "Variables in the current scope:\n",
    "\n",
    "   Attr Name                   Size                     Bytes  Class\n",
    "   ==== ====                   ====                     =====  ===== \n",
    "        Theta1                25x401                    80200  double\n",
    "        Theta2                10x26                      2080  double\n",
    "        X                   5000x400                 16000000  double\n",
    "        ans                    1x1                          8  double\n",
    "        hidden_layer_size      1x1                          8  double\n",
    "        input_layer_size       1x1                          8  double\n",
    "        m                      1x1                          8  double\n",
    "        nn_params          10285x1                      82280  double\n",
    "        num_labels             1x1                          8  double\n",
    "        y                   5000x1                      40000  double\n",
    "%}\n",
    "\n",
    "% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "% for our 2 layer neural network\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "% Setup some useful variables\n",
    "m = size(X, 1); % number of rows (images)\n",
    "\n",
    "alpha_one = [ones(m,1), X]; % 5000x401\n",
    "\n",
    "alpha_two = [ones(m,1), sigmoid(alpha_one * Theta1')]; % 5000x26\n",
    "\n",
    "alpha_three = sigmoid(alpha_two * Theta2'); % 5000x10\n",
    "         \n",
    "% You need to return the following variables correctly \n",
    "J = 0;\n",
    "Theta1_grad = zeros(size(Theta1));\n",
    "Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "%{\n",
    "L  = total number of layers in the network\n",
    "sl = number of units (not counting bias unit) in layer l\n",
    "K = number of output units/classes\n",
    "%}\n",
    "\n",
    "K = num_labels;\n",
    "\n",
    "% h_Theta = sigmoid(X)\n",
    "h_Theta = alpha_three;\n",
    "\n",
    "y_matrix = (1:num_labels)==y;\n",
    "\n",
    "size(y_matrix)\n",
    "size(h_Theta)\n",
    "\n",
    "regularization_term = (lambda/(2*m))*(sum(sum(Theta1(:,2:end).^2))+sum(sum(Theta2(:,2:end).^2)))\n",
    "\n",
    "\n",
    "% J=(1/(m))*sum(-y_matrix'*log(h_Theta)-(1-y_matrix)'*log(1-h_Theta));\n",
    "\n",
    "% J = (-1/m)*sum((y_matrix'*log(h_Theta)) + (1-y_matrix)'*log(1-h_Theta));\n",
    "% J = (-1/m) * sum(sum((y_matrix.*log(h_Theta))+((1-y_matrix).*log(1-h_Theta))));  %scalar\n",
    "\n",
    "J = (-1/m) * sum(sum((y_matrix.*log(h_Theta))+((1-y_matrix).*log(1-h_Theta)))) + regularization_term;\n",
    "\n",
    "\n",
    "\n",
    "% J\n",
    "\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "% Instructions: You should complete the code by working through the\n",
    "%               following parts.\n",
    "%\n",
    "% Part 1: Feedforward the neural network and return the cost in the\n",
    "%         variable J. After implementing Part 1, you can verify that your\n",
    "%         cost function computation is correct by verifying the cost\n",
    "%         computed in ex4.m\n",
    "%\n",
    "% Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "%         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "%         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "%         that your implementation is correct by running checkNNGradients\n",
    "%\n",
    "%         Note: The vector y passed into the function is a vector of labels\n",
    "%               containing values from 1..K. You need to map this vector into a \n",
    "%               binary vector of 1's and 0's to be used with the neural network\n",
    "%               cost function.\n",
    "%\n",
    "%         Hint: We recommend implementing backpropagation using a for-loop\n",
    "%               over the training examples if you are implementing it for the \n",
    "%               first time.\n",
    "%\n",
    "% Part 3: Implement regularization with the cost function and gradients.\n",
    "%\n",
    "%         Hint: You can implement this around the code for\n",
    "%               backpropagation. That is, you can compute the gradients for\n",
    "%               the regularization separately and then add them to Theta1_grad\n",
    "%               and Theta2_grad from Part 2.\n",
    "%\n",
    "\n",
    "\n",
    "% Error Calculation for Backpropagation\n",
    "\n",
    "delta_3 = alpha_three - y_matrix % 5000x10 -> difference in hypothesis and actual output\n",
    "\n",
    "delta_2 = (Theta2' * delta_three) .* (1 - alpha_2)\n",
    "\n",
    "delta_1 = (Theta1' * delta_two) .* (1 - )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% -------------------------------------------------------------\n",
    "\n",
    "% =========================================================================\n",
    "\n",
    "% Unroll gradients\n",
    "grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "lambda = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =  1.00000\n"
     ]
    }
   ],
   "source": [
    "sigmoid(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Vec = (0:num_labels)==y;\n",
    "\n",
    "y_Vec(1:10,:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables in the current scope:\n",
      "\n",
      "   Attr Name                   Size                     Bytes  Class\n",
      "   ==== ====                   ====                     =====  ===== \n",
      "        J                      1x5000                   40000  double\n",
      "        Theta1                25x401                    80200  double\n",
      "        Theta2                10x26                      2080  double\n",
      "        X                   5000x400                 16000000  double\n",
      "        ans                   10x11                       110  logical\n",
      "        hidden_layer_size      1x1                          8  double\n",
      "        input_layer_size       1x1                          8  double\n",
      "        lambda                 1x1                          8  double\n",
      "        m                      1x1                          8  double\n",
      "        nn_params          10285x1                      82280  double\n",
      "        num_labels             1x1                          8  double\n",
      "        y                   5000x1                      40000  double\n",
      "        y_Vec               5000x11                     55000  logical\n",
      "\n",
      "Total is 2085685 elements using 16299710 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
